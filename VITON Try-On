{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname,_)   #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:05:00.564777Z","iopub.execute_input":"2025-02-04T20:05:00.565104Z","iopub.status.idle":"2025-02-04T20:08:32.980429Z","shell.execute_reply.started":"2025-02-04T20:05:00.565059Z","shell.execute_reply":"2025-02-04T20:08:32.979119Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input ['high-resolution-viton-zalando-dataset']\n/kaggle/input/high-resolution-viton-zalando-dataset ['test', 'train']\n/kaggle/input/high-resolution-viton-zalando-dataset/test ['image-densepose', 'agnostic-v3.2', 'openpose_img', 'image-parse-agnostic-v3.2', 'openpose_json', 'cloth', 'cloth-mask', 'image-parse-v3', 'image']\n/kaggle/input/high-resolution-viton-zalando-dataset/test/image-densepose []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/agnostic-v3.2 []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/openpose_img []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/image-parse-agnostic-v3.2 []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/openpose_json []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/cloth []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/cloth-mask []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/image-parse-v3 []\n/kaggle/input/high-resolution-viton-zalando-dataset/test/image []\n/kaggle/input/high-resolution-viton-zalando-dataset/train ['image-densepose', 'agnostic-v3.2', 'openpose_img', 'image-parse-agnostic-v3.2', 'openpose_json', 'cloth', 'cloth-mask', 'image-parse-v3', 'image']\n/kaggle/input/high-resolution-viton-zalando-dataset/train/image-densepose []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/agnostic-v3.2 []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/openpose_img []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/image-parse-agnostic-v3.2 []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/openpose_json []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/cloth []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/cloth-mask []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/image-parse-v3 []\n/kaggle/input/high-resolution-viton-zalando-dataset/train/image []\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pandas as  pd\nimport matplotlib.pyplot as plt\nimport json\nfrom tensorflow import keras\nimport os\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.applications import VGG19","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:17:52.534936Z","iopub.execute_input":"2025-02-04T20:17:52.535350Z","iopub.status.idle":"2025-02-04T20:17:52.548286Z","shell.execute_reply.started":"2025-02-04T20:17:52.535317Z","shell.execute_reply":"2025-02-04T20:17:52.547040Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"cloth_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/cloth/\"\nmask_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/cloth-mask/\"\nimage_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/image/\"\nopenpose_json_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/openpose_json/\"\nimage_parse_v3_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/image-parse-v3/\"\nopenpose_img=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/openpose_img/\"\nagnostic_parse_v3_dir=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/agnostic-v3.2/\"\ndencepose_img=\"/kaggle/input/high-resolution-viton-zalando-dataset/train/image-densepose/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:12:12.578424Z","iopub.execute_input":"2025-02-04T20:12:12.578909Z","iopub.status.idle":"2025-02-04T20:12:12.584990Z","shell.execute_reply.started":"2025-02-04T20:12:12.578873Z","shell.execute_reply":"2025-02-04T20:12:12.583776Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"cloth_paths = [os.path.join(cloth_dir, file) for file in os.listdir(cloth_dir)]\nmask_paths = [os.path.join(mask_dir, file) for file in os.listdir(mask_dir)]\nimage_paths = [os.path.join(image_dir, file) for file in os.listdir(image_dir)]\njson_paths = [os.path.join(openpose_json_dir, file) for file in os.listdir(openpose_json_dir)]\nimage_parse_v3_dir_path = [os.path.join(image_parse_v3_dir, file) for file in os.listdir(image_parse_v3_dir)]\nopenpose_img_path = [os.path.join(openpose_img, file) for file in os.listdir(openpose_img)]\nagnostic_parse_v3_dir_path = [os.path.join(agnostic_parse_v3_dir, file) for file in os.listdir(agnostic_parse_v3_dir)]\ndencepose_img_path = [os.path.join(dencepose_img, file) for file in os.listdir(dencepose_img)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:12:45.480525Z","iopub.execute_input":"2025-02-04T20:12:45.481302Z","iopub.status.idle":"2025-02-04T20:12:45.615884Z","shell.execute_reply.started":"2025-02-04T20:12:45.481221Z","shell.execute_reply":"2025-02-04T20:12:45.614830Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"cloth_paths.sort()\nmask_paths.sort()\nimage_paths.sort()\njson_paths.sort()\nimage_parse_v3_dir_path.sort()\nopenpose_img_path.sort()\nagnostic_parse_v3_dir_path.sort()\ndencepose_img_path.sort()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:12:59.373201Z","iopub.execute_input":"2025-02-04T20:12:59.373643Z","iopub.status.idle":"2025-02-04T20:12:59.412937Z","shell.execute_reply.started":"2025-02-04T20:12:59.373603Z","shell.execute_reply":"2025-02-04T20:12:59.411732Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Mask Model","metadata":{}},{"cell_type":"code","source":"def preprocessing(image_path,mask_path):\n    images = []\n    masks = []\n    for image_path, mask_path in zip(image_path,mask_path):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, (512, 512))\n        \n        mask = tf.io.read_file(mask_path)\n        mask = tf.image.decode_png(mask, channels=1)\n        mask = tf.image.resize(mask, (512, 512))\n    \n        images.append(image.numpy()/255.0)\n        masks.append(mask.numpy()/255.0)\n    images = np.array(images)\n    masks = np.array(masks)\n\n    print(images.shape, masks.shape)\n    return images,masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:49:36.025402Z","iopub.execute_input":"2024-11-21T11:49:36.025803Z","iopub.status.idle":"2024-11-21T11:49:36.035169Z","shell.execute_reply.started":"2024-11-21T11:49:36.025763Z","shell.execute_reply":"2024-11-21T11:49:36.033943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images, train_masks = preprocessing(cloth_paths[:750],mask_paths[:750])\ntest_images, test_masks = preprocessing(cloth_paths[750:1000],mask_paths[750:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:50:24.896021Z","iopub.execute_input":"2024-11-21T11:50:24.896742Z","iopub.status.idle":"2024-11-21T11:51:02.154915Z","shell.execute_reply.started":"2024-11-21T11:50:24.896706Z","shell.execute_reply":"2024-11-21T11:51:02.153757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(train_images[0])\nplt.title(\"Train Image\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:51:42.613092Z","iopub.execute_input":"2024-11-21T11:51:42.613984Z","iopub.status.idle":"2024-11-21T11:51:42.985920Z","shell.execute_reply.started":"2024-11-21T11:51:42.613930Z","shell.execute_reply":"2024-11-21T11:51:42.984811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(train_masks[0])\nplt.title(\"Train Masks\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:52:13.915123Z","iopub.execute_input":"2024-11-21T11:52:13.915538Z","iopub.status.idle":"2024-11-21T11:52:14.221148Z","shell.execute_reply.started":"2024-11-21T11:52:13.915501Z","shell.execute_reply":"2024-11-21T11:52:14.219994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clothing_parser(input_shape=(512, 512, 3)):\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Encoder\n    x = keras.layers.Conv2D(64, (2, 2), strides=2, padding='same', activation='relu')(inputs)\n    x = keras.layers.Conv2D(128, (2, 2), strides=2, padding='same', activation='relu')(x)\n    x = keras.layers.Conv2D(256, (2, 2), strides=2, padding='same', activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv2D(512, (2, 2), strides=2, padding='same', activation='relu')(x)\n    \n    # Decoder\n    x = keras.layers.Conv2DTranspose(256, (2, 2), strides=2, padding='same', activation='relu')(x)\n    x = keras.layers.Conv2DTranspose(128, (2,2), strides=2, padding='same', activation='relu')(x)\n    x = keras.layers.Conv2DTranspose(32, (2, 2), strides=2, padding='same', activation='sigmoid')(x)\n    x = keras.layers.BatchNormalization()(x)\n    outputs = keras.layers.Conv2DTranspose(1, (2, 2), strides=2, padding='same', activation='sigmoid')(x)\n    \n    return keras.models.Model(inputs, outputs, name='ClothingParser')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:53:03.923477Z","iopub.execute_input":"2024-11-21T11:53:03.924331Z","iopub.status.idle":"2024-11-21T11:53:03.933586Z","shell.execute_reply.started":"2024-11-21T11:53:03.924244Z","shell.execute_reply":"2024-11-21T11:53:03.932070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=clothing_parser(input_shape=(512,512,3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:53:13.251521Z","iopub.execute_input":"2024-11-21T11:53:13.251891Z","iopub.status.idle":"2024-11-21T11:53:13.432489Z","shell.execute_reply.started":"2024-11-21T11:53:13.251860Z","shell.execute_reply":"2024-11-21T11:53:13.431542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:53:21.683929Z","iopub.execute_input":"2024-11-21T11:53:21.684342Z","iopub.status.idle":"2024-11-21T11:53:21.710967Z","shell.execute_reply.started":"2024-11-21T11:53:21.684307Z","shell.execute_reply":"2024-11-21T11:53:21.710032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True,show_layer_names=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:54:01.731276Z","iopub.execute_input":"2024-11-21T11:54:01.732079Z","iopub.status.idle":"2024-11-21T11:54:02.448577Z","shell.execute_reply.started":"2024-11-21T11:54:01.732045Z","shell.execute_reply":"2024-11-21T11:54:02.447454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics=['accuracy','mae'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:56:39.368863Z","iopub.execute_input":"2024-11-21T11:56:39.369784Z","iopub.status.idle":"2024-11-21T11:56:39.379748Z","shell.execute_reply.started":"2024-11-21T11:56:39.369748Z","shell.execute_reply":"2024-11-21T11:56:39.378463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(train_images, train_masks,batch_size=8,epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:56:41.122853Z","iopub.execute_input":"2024-11-21T11:56:41.123488Z","iopub.status.idle":"2024-11-21T12:34:16.977368Z","shell.execute_reply.started":"2024-11-21T11:56:41.123451Z","shell.execute_reply":"2024-11-21T12:34:16.975672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.predict(test_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:37:14.228669Z","iopub.execute_input":"2024-11-21T12:37:14.229112Z","iopub.status.idle":"2024-11-21T12:37:37.998372Z","shell.execute_reply.started":"2024-11-21T12:37:14.229074Z","shell.execute_reply":"2024-11-21T12:37:37.997377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:39:37.963581Z","iopub.execute_input":"2024-11-21T12:39:37.964468Z","iopub.status.idle":"2024-11-21T12:39:37.974827Z","shell.execute_reply.started":"2024-11-21T12:39:37.964428Z","shell.execute_reply":"2024-11-21T12:39:37.973715Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i  in  range(4):\n    plt.subplot(2,4,i+1)\n    plt.title(\"Images\")\n    plt.imshow(test_images[i+5],cmap='nipy_spectral')\n    plt.subplot(2,4,i+5)\n    plt.title(\"Predicted Mask\")\n    plt.imshow(history[i+5],cmap='nipy_spectral')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:41:02.407289Z","iopub.execute_input":"2024-11-21T12:41:02.407735Z","iopub.status.idle":"2024-11-21T12:41:04.084053Z","shell.execute_reply.started":"2024-11-21T12:41:02.407697Z","shell.execute_reply":"2024-11-21T12:41:04.082806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VITON Model","metadata":{}},{"cell_type":"markdown","source":"Input Feature Extraction module","metadata":{}},{"cell_type":"code","source":"class GarmentFeatureExtractor(tf.keras.Model):\n    def __init__(self, input_shape):\n        super(GarmentFeatureExtractor, self).__init__()\n        self.conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n        self.pool1 = layers.MaxPooling2D((2, 2))\n        self.conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')\n        self.pool2 = layers.MaxPooling2D((2, 2))\n        self.conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')\n        self.global_pool = layers.GlobalAveragePooling2D()\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = self.global_pool(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:15:59.981350Z","iopub.execute_input":"2025-02-04T20:15:59.982470Z","iopub.status.idle":"2025-02-04T20:16:00.068272Z","shell.execute_reply.started":"2025-02-04T20:15:59.982425Z","shell.execute_reply":"2025-02-04T20:16:00.067270Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Wrapping Module","metadata":{}},{"cell_type":"code","source":"class WarpingModule(tf.keras.Model):\n    def __init__(self, img_size):\n        super(WarpingModule, self).__init__()\n        self.img_size = img_size\n        self.dense1 = layers.Dense(512, activation='relu')\n        self.dense2 = layers.Dense(img_size[0] * img_size[1] * 2, activation='sigmoid')\n\n    def call(self, inputs):\n        cloth_features, human_parse, agnostic_parse = inputs\n        flattened_parse = layers.Flatten()(layers.GlobalAveragePooling2D()(human_parse))\n        x = layers.Concatenate()([flattened_parse, cloth_features])\n        x = self.dense1(x)\n        warp_field = self.dense2(x)\n        return warp_field","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:16:01.502654Z","iopub.execute_input":"2025-02-04T20:16:01.503047Z","iopub.status.idle":"2025-02-04T20:16:01.510913Z","shell.execute_reply.started":"2025-02-04T20:16:01.503012Z","shell.execute_reply":"2025-02-04T20:16:01.509434Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Rendering Module","metadata":{}},{"cell_type":"code","source":"class RenderingModule(tf.keras.Model):\n    def __init__(self):\n        super(RenderingModule, self).__init__()\n        self.concat = layers.Concatenate(axis=-1)\n        self.conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n        self.conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')\n        self.conv3 = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n\n    def call(self, inputs):\n        warped_cloth, human_parse = inputs\n        x = self.concat([warped_cloth, human_parse])\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:16:02.882789Z","iopub.execute_input":"2025-02-04T20:16:02.883207Z","iopub.status.idle":"2025-02-04T20:16:02.891186Z","shell.execute_reply.started":"2025-02-04T20:16:02.883174Z","shell.execute_reply":"2025-02-04T20:16:02.889902Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"VITON Gan\n","metadata":{}},{"cell_type":"code","source":"class WarpGAN(tf.keras.Model):\n    def __init__(self, img_size):\n        super(WarpGAN, self).__init__()\n        self.img_size = img_size\n        self.garment_extractor = GarmentFeatureExtractor(input_shape=img_size)\n        self.warping_module = WarpingModule(img_size)\n        self.rendering_module = RenderingModule()\n\n    def call(self, inputs):\n        input_cloth, input_human_parse, input_agnostic_parse = inputs\n        cloth_features = self.garment_extractor(input_cloth)\n        warp_field = self.warping_module([cloth_features, input_human_parse, input_agnostic_parse])\n        warped_cloth = tf.image.resize(input_cloth, (self.img_size[0], self.img_size[1]))  # Mock warping placeholder\n        output_image = self.rendering_module([warped_cloth, input_human_parse])\n        return output_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:16:04.483690Z","iopub.execute_input":"2025-02-04T20:16:04.484068Z","iopub.status.idle":"2025-02-04T20:16:04.491699Z","shell.execute_reply.started":"2025-02-04T20:16:04.484036Z","shell.execute_reply":"2025-02-04T20:16:04.490207Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define input dimensions\nIMG_SIZE = (256, 256, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:16:06.403601Z","iopub.execute_input":"2025-02-04T20:16:06.404365Z","iopub.status.idle":"2025-02-04T20:16:06.409155Z","shell.execute_reply.started":"2025-02-04T20:16:06.404304Z","shell.execute_reply":"2025-02-04T20:16:06.407928Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Instantiate and summarize the model\nwarpgan_model = WarpGAN(img_size=IMG_SIZE)\ninputs = [\n    tf.keras.Input(shape=IMG_SIZE, name=\"input_cloth\"),\n    tf.keras.Input(shape=IMG_SIZE, name=\"input_human_parse\"),\n    tf.keras.Input(shape=IMG_SIZE, name=\"input_agnostic_parse\")\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:17:01.349028Z","iopub.execute_input":"2025-02-04T20:17:01.349845Z","iopub.status.idle":"2025-02-04T20:17:01.374986Z","shell.execute_reply.started":"2025-02-04T20:17:01.349805Z","shell.execute_reply":"2025-02-04T20:17:01.373783Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"outputs = warpgan_model(inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:17:08.072835Z","iopub.execute_input":"2025-02-04T20:17:08.073746Z","iopub.status.idle":"2025-02-04T20:17:09.331062Z","shell.execute_reply.started":"2025-02-04T20:17:08.073703Z","shell.execute_reply":"2025-02-04T20:17:09.329900Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"WarpGAN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:17:13.797121Z","iopub.execute_input":"2025-02-04T20:17:13.797610Z","iopub.status.idle":"2025-02-04T20:17:13.806414Z","shell.execute_reply.started":"2025-02-04T20:17:13.797567Z","shell.execute_reply":"2025-02-04T20:17:13.805197Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:17:14.484803Z","iopub.execute_input":"2025-02-04T20:17:14.485174Z","iopub.status.idle":"2025-02-04T20:17:14.511739Z","shell.execute_reply.started":"2025-02-04T20:17:14.485145Z","shell.execute_reply":"2025-02-04T20:17:14.510656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"WarpGAN\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"WarpGAN\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_cloth         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_human_parse   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_agnostic_par… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ warp_gan_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │ \u001b[38;5;34m67,824,707\u001b[0m │ input_cloth[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mWarpGAN\u001b[0m)           │ \u001b[38;5;34m3\u001b[0m)                │            │ input_human_pars… │\n│                     │                   │            │ input_agnostic_p… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_cloth         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_human_parse   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_agnostic_par… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ warp_gan_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">67,824,707</span> │ input_cloth[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">WarpGAN</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │ input_human_pars… │\n│                     │                   │            │ input_agnostic_p… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,824,707\u001b[0m (258.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,824,707</span> (258.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,824,707\u001b[0m (258.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,824,707</span> (258.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def build_discriminator(input_shape):\n    inputs = tf.keras.Input(shape=input_shape)\n\n    x = layers.Conv2D(64, (4, 4), strides=2, padding='same')(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(128, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(256, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(512, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Flatten()(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n\n    return tf.keras.Model(inputs, x, name=\"Discriminator\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:18:24.124633Z","iopub.execute_input":"2025-02-04T20:18:24.125075Z","iopub.status.idle":"2025-02-04T20:18:24.136325Z","shell.execute_reply.started":"2025-02-04T20:18:24.125040Z","shell.execute_reply":"2025-02-04T20:18:24.134894Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Instantiate WarpGAN and discriminator\nwarpgan_generator = WarpGAN(img_size=(256, 256, 3))\ndiscriminator = build_discriminator((256, 256, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:18:29.651433Z","iopub.execute_input":"2025-02-04T20:18:29.651853Z","iopub.status.idle":"2025-02-04T20:18:29.773554Z","shell.execute_reply.started":"2025-02-04T20:18:29.651818Z","shell.execute_reply":"2025-02-04T20:18:29.772263Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Pre-trained VGG for perceptual loss\nvgg = VGG19(include_top=False, weights=\"imagenet\")\nvgg = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer(\"block5_conv4\").output)\nvgg.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:18:33.880933Z","iopub.execute_input":"2025-02-04T20:18:33.881380Z","iopub.status.idle":"2025-02-04T20:18:34.879662Z","shell.execute_reply.started":"2025-02-04T20:18:33.881340Z","shell.execute_reply":"2025-02-04T20:18:34.878678Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Optimizers and loss functions\ngen_optimizer = Adam(1e-4, beta_1=0.5)\ndisc_optimizer = Adam(1e-4, beta_1=0.5)\nbce_loss = BinaryCrossentropy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:18:39.905955Z","iopub.execute_input":"2025-02-04T20:18:39.906812Z","iopub.status.idle":"2025-02-04T20:18:39.920962Z","shell.execute_reply.started":"2025-02-04T20:18:39.906772Z","shell.execute_reply":"2025-02-04T20:18:39.919943Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Perceptual loss function\ndef perceptual_loss(y_true, y_pred):\n    y_true_features = vgg(y_true)\n    y_pred_features = vgg(y_pred)\n    return tf.reduce_mean(tf.abs(y_true_features - y_pred_features))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:18:45.610975Z","iopub.execute_input":"2025-02-04T20:18:45.611654Z","iopub.status.idle":"2025-02-04T20:18:45.617186Z","shell.execute_reply.started":"2025-02-04T20:18:45.611611Z","shell.execute_reply":"2025-02-04T20:18:45.616020Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}